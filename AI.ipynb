{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28",
      "mount_file_id": "1yDrk1skEtMmM3PEcq8pEbLOJkrUZyPkN",
      "authorship_tag": "ABX9TyPn8Ki/X/620rfOeCX0J+Qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malix-Labs/EFREI-2026-I2-S7-AI-TP-1/blob/main/AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Reminder Questions"
      ],
      "metadata": {
        "id": "UJaJ1jwz5Bn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Difference between Linear and Logistic Regression:**\n",
        "\n",
        "* **Linear Regression** predicts a **continuous** output variable (y) based on a linear relationship with input variables (x). For example, predicting house prices based on size, location, etc.\n",
        "* **Logistic Regression** predicts a **categorical** output variable (y), typically binary (0 or 1), by estimating the probability of belonging to a certain category. For example, predicting whether an email is spam or not.\n",
        "\n",
        "**2. Loss Function and Error Function Shape:**\n",
        "\n",
        "* **Loss Function:**  A function that measures the difference between the predicted output and the actual output. We minimize the loss function during training to improve the model's accuracy.\n",
        "* **Error Function Shape:**\n",
        "    * **Linear Regression:** Uses **Mean Squared Error (MSE)**. The shape is a parabola, with a single global minimum. The gradient descent algorithm aims to find this minimum.\n",
        "    * **Logistic Regression:** Uses **Log Loss (Binary Cross-Entropy)**. The shape is sigmoidal and convex. Like MSE, it also has a single global minimum that we try to find during training.\n",
        "\n",
        "**3. Core Algorithm:**\n",
        "\n",
        "* Both linear and logistic regression use variations of the **Gradient Descent** algorithm at their core. Gradient descent iteratively adjusts the model's parameters (weights and bias) to minimize the loss function. However, the update rules and loss functions used differ between the two methods due to the different types of output variables they predict.\n",
        "\n",
        "**4. Convergency:**\n",
        "\n",
        "* **Convergency** in both cases means that the algorithm has found a set of model parameters (weights and bias) where the loss function is minimized.  \n",
        "    * In other words, further iterations of the algorithm will not significantly improve the model's performance.\n",
        "    *  It indicates that the model has learned the relationship between the input and output variables as best as possible with the given data and chosen hyperparameters (like learning rate)."
      ],
      "metadata": {
        "id": "SNnkNING5Liw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "zq-eg0cA1LWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#------------ Parameters -----------#\n",
        "'''Define your hyper-parameters in this place'''\n",
        "\n",
        "# --------- Functions  ---------------#\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self):\n",
        "        ''' Initialization of Linear Regression \"hyper-parameters\"  '''\n",
        "\n",
        "    def mse(self,y_true, y_pred):\n",
        "        ''' Mean Squared Error : returns a scalar value '''\n",
        "\n",
        "    def train_GDA(self,X, y):\n",
        "        '''\n",
        "        - Training function    : GDA - Gradient Descent Algorithm\n",
        "        - Stopping criteration : Maxiterations\n",
        "        - Each feature needs to correspond to a weight\n",
        "        '''\n",
        "\n",
        "        print(\"Weights = \",self.weights,\" and Bias = \",self.bias)\n",
        "\n",
        "    def predict(self,X):\n",
        "        ''' The prediction function : returns y_predicted '''\n",
        "\n",
        "    def plot_results(self, X , y , y_pred_line, name):\n",
        "        '''\n",
        "        -plot data as scatter points\n",
        "        -plots linear regression model\n",
        "        -function only suppots 1 feature !\n",
        "        -save results in \"pdf\" format\n",
        "        '''\n",
        "\n",
        "    def plot_loss_surface(self, X, y):\n",
        "        '''\n",
        "        -Function plots the 3D space of the loss function : weigth1 axis, weight2 axis and loss axis\n",
        "        -Plots the optimal trajectory, from random initial weights, to optimal weights\n",
        "        '''\n",
        "\n",
        "# --------- Testing ---------------#\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    '''\n",
        "    -You need to call all your function in here.\n",
        "    -To code your model, you only need \"numpy\" !\n",
        "    -Scklearn is only used to import dataSets  !\n",
        "\n",
        "    1) Import your data set (from scklearn) with \"make_regression\" function\n",
        "    2) Pre_process them if needed\n",
        "    3) Train your model\n",
        "    4) Teste your model and display the mean squared error\n",
        "    5) Plot the regression line\n",
        "    - Repeat the five steps on different parameters of your dataset, on different hyper-parameters.\n",
        "    - Compare Results\n",
        "    6) Plot optimal trajctory of GDA algorithm in loss landscape space\n",
        "    '''"
      ],
      "metadata": {
        "id": "0LRi5lbJ2umG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "fHtNRmtv1Jmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#------------ Parameters -----------#\n",
        "'''Define your hyper-parameters in this place'''\n",
        "\n",
        "# --------- Functions  ---------------#\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self):\n",
        "        ''' Initialization of Linear Regression \"hyper-parameters\"  '''\n",
        "\n",
        "    def sigmoid(self,z):\n",
        "        '''sigmoid activation function : returns a real number'''\n",
        "\n",
        "    def train_GDA(self,X, y):\n",
        "        '''\n",
        "        - Training function    : GDA - Gradient Descent Algorithm\n",
        "        - Stopping criteration : Maxiterations\n",
        "        - Each feature needs to correspond to a weight\n",
        "        '''\n",
        "\n",
        "        print(\"Weights = \",self.weights,\" and Bias = \",self.bias)\n",
        "\n",
        "    def accuracy(self,y_true, y_pred):\n",
        "        '''Accuracy function : y' vs y : returns % accuracy '''\n",
        "\n",
        "    def predict(self,X):\n",
        "        '''\n",
        "        - The prdiction function : returns y_predicted\n",
        "        - Applying a threshold of 0.5\n",
        "        '''\n",
        "\n",
        "    def plot_results(self, y_target, y_pred, accuracy, name):\n",
        "        '''\n",
        "        -plot a barplot for y_target\n",
        "        -plot a barplot for y_pred\n",
        "        -show accuracy of the model in title\n",
        "        -save results in \"pdf\" format\n",
        "        '''\n",
        "\n",
        "# --------- Testing ---------------#\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    '''\n",
        "    -You need to call all your function in here.\n",
        "    -To code your model, you only need \"numpy\" !\n",
        "    -Scklearn is only used to import dataSets  !\n",
        "\n",
        "    1) Import your data set (from scklearn) with \"make_regression\" function\n",
        "    2) Pre_process them if needed\n",
        "    3) Train your model\n",
        "    4) Teste your model and display the mean squared error\n",
        "    5) Plot results\n",
        "    - Repeat the five steps on different parameters of your dataset, on different hyper-parameters.\n",
        "    - Compare Results\n",
        "    '''"
      ],
      "metadata": {
        "id": "wUaeTfXk22DA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}